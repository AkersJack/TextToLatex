{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AkersJack/TextToLatex/blob/main/Text2Latex_train.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Smw5ao4U8FVR",
        "outputId": "3f1fcd8a-bcfe-4ee0-a757-a7c0522ea7b9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting causal-conv1d\n",
            "  Downloading causal_conv1d-1.2.0.post2.tar.gz (7.1 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (from causal-conv1d) (2.2.1+cu121)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from causal-conv1d) (24.0)\n",
            "Collecting ninja (from causal-conv1d)\n",
            "  Downloading ninja-1.11.1.1-py2.py3-none-manylinux1_x86_64.manylinux_2_5_x86_64.whl (307 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m307.2/307.2 kB\u001b[0m \u001b[31m8.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch->causal-conv1d) (3.13.4)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch->causal-conv1d) (4.11.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch->causal-conv1d) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch->causal-conv1d) (3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch->causal-conv1d) (3.1.3)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch->causal-conv1d) (2023.6.0)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.1.105 (from torch->causal-conv1d)\n",
            "  Using cached nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (23.7 MB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.1.105 (from torch->causal-conv1d)\n",
            "  Using cached nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (823 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.1.105 (from torch->causal-conv1d)\n",
            "  Using cached nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (14.1 MB)\n",
            "Collecting nvidia-cudnn-cu12==8.9.2.26 (from torch->causal-conv1d)\n",
            "  Using cached nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl (731.7 MB)\n",
            "Collecting nvidia-cublas-cu12==12.1.3.1 (from torch->causal-conv1d)\n",
            "  Using cached nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl (410.6 MB)\n",
            "Collecting nvidia-cufft-cu12==11.0.2.54 (from torch->causal-conv1d)\n",
            "  Using cached nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl (121.6 MB)\n",
            "Collecting nvidia-curand-cu12==10.3.2.106 (from torch->causal-conv1d)\n",
            "  Using cached nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl (56.5 MB)\n",
            "Collecting nvidia-cusolver-cu12==11.4.5.107 (from torch->causal-conv1d)\n",
            "  Using cached nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl (124.2 MB)\n",
            "Collecting nvidia-cusparse-cu12==12.1.0.106 (from torch->causal-conv1d)\n",
            "  Using cached nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl (196.0 MB)\n",
            "Collecting nvidia-nccl-cu12==2.19.3 (from torch->causal-conv1d)\n",
            "  Using cached nvidia_nccl_cu12-2.19.3-py3-none-manylinux1_x86_64.whl (166.0 MB)\n",
            "Collecting nvidia-nvtx-cu12==12.1.105 (from torch->causal-conv1d)\n",
            "  Using cached nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (99 kB)\n",
            "Requirement already satisfied: triton==2.2.0 in /usr/local/lib/python3.10/dist-packages (from torch->causal-conv1d) (2.2.0)\n",
            "Collecting nvidia-nvjitlink-cu12 (from nvidia-cusolver-cu12==11.4.5.107->torch->causal-conv1d)\n",
            "  Using cached nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch->causal-conv1d) (2.1.5)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch->causal-conv1d) (1.3.0)\n",
            "Building wheels for collected packages: causal-conv1d\n",
            "  Building wheel for causal-conv1d (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for causal-conv1d: filename=causal_conv1d-1.2.0.post2-cp310-cp310-linux_x86_64.whl size=45616902 sha256=9fffdd36640e6e52e9c3d2ad6ac5483f109cecc2fdeedfc5767487893f8ccae2\n",
            "  Stored in directory: /root/.cache/pip/wheels/eb/8f/52/646c36c652677964016059dd0845960d162668451aeaaf4533\n",
            "Successfully built causal-conv1d\n",
            "Installing collected packages: ninja, nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12, causal-conv1d\n",
            "Successfully installed causal-conv1d-1.2.0.post2 ninja-1.11.1.1 nvidia-cublas-cu12-12.1.3.1 nvidia-cuda-cupti-cu12-12.1.105 nvidia-cuda-nvrtc-cu12-12.1.105 nvidia-cuda-runtime-cu12-12.1.105 nvidia-cudnn-cu12-8.9.2.26 nvidia-cufft-cu12-11.0.2.54 nvidia-curand-cu12-10.3.2.106 nvidia-cusolver-cu12-11.4.5.107 nvidia-cusparse-cu12-12.1.0.106 nvidia-nccl-cu12-2.19.3 nvidia-nvjitlink-cu12-12.4.127 nvidia-nvtx-cu12-12.1.105\n",
            "Collecting mamba-ssm\n",
            "  Downloading mamba_ssm-1.2.0.post1.tar.gz (34 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (from mamba-ssm) (2.2.1+cu121)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from mamba-ssm) (24.0)\n",
            "Requirement already satisfied: ninja in /usr/local/lib/python3.10/dist-packages (from mamba-ssm) (1.11.1.1)\n",
            "Collecting einops (from mamba-ssm)\n",
            "  Downloading einops-0.8.0-py3-none-any.whl (43 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.2/43.2 kB\u001b[0m \u001b[31m5.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: triton in /usr/local/lib/python3.10/dist-packages (from mamba-ssm) (2.2.0)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (from mamba-ssm) (4.40.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch->mamba-ssm) (3.13.4)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch->mamba-ssm) (4.11.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch->mamba-ssm) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch->mamba-ssm) (3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch->mamba-ssm) (3.1.3)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch->mamba-ssm) (2023.6.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch->mamba-ssm) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch->mamba-ssm) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch->mamba-ssm) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /usr/local/lib/python3.10/dist-packages (from torch->mamba-ssm) (8.9.2.26)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /usr/local/lib/python3.10/dist-packages (from torch->mamba-ssm) (12.1.3.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /usr/local/lib/python3.10/dist-packages (from torch->mamba-ssm) (11.0.2.54)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /usr/local/lib/python3.10/dist-packages (from torch->mamba-ssm) (10.3.2.106)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /usr/local/lib/python3.10/dist-packages (from torch->mamba-ssm) (11.4.5.107)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /usr/local/lib/python3.10/dist-packages (from torch->mamba-ssm) (12.1.0.106)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.19.3 in /usr/local/lib/python3.10/dist-packages (from torch->mamba-ssm) (2.19.3)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch->mamba-ssm) (12.1.105)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.10/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch->mamba-ssm) (12.4.127)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.19.3 in /usr/local/lib/python3.10/dist-packages (from transformers->mamba-ssm) (0.20.3)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers->mamba-ssm) (1.25.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers->mamba-ssm) (6.0.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers->mamba-ssm) (2023.12.25)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers->mamba-ssm) (2.31.0)\n",
            "Requirement already satisfied: tokenizers<0.20,>=0.19 in /usr/local/lib/python3.10/dist-packages (from transformers->mamba-ssm) (0.19.1)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers->mamba-ssm) (0.4.3)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers->mamba-ssm) (4.66.2)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch->mamba-ssm) (2.1.5)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers->mamba-ssm) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers->mamba-ssm) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers->mamba-ssm) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers->mamba-ssm) (2024.2.2)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch->mamba-ssm) (1.3.0)\n",
            "Building wheels for collected packages: mamba-ssm\n",
            "  Building wheel for mamba-ssm (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for mamba-ssm: filename=mamba_ssm-1.2.0.post1-cp310-cp310-linux_x86_64.whl size=137750683 sha256=b264292652a34fb9dd0ce880a34a4407ba7256a3338388d056769ec29a4581c9\n",
            "  Stored in directory: /root/.cache/pip/wheels/22/6e/60/ddd5c574b5793a30028f2cabdacd2a3ec2276edaaa8c00fd35\n",
            "Successfully built mamba-ssm\n",
            "Installing collected packages: einops, mamba-ssm\n",
            "Successfully installed einops-0.8.0 mamba-ssm-1.2.0.post1\n",
            "Collecting datasets\n",
            "  Downloading datasets-2.19.0-py3-none-any.whl (542 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m542.0/542.0 kB\u001b[0m \u001b[31m10.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from datasets) (3.13.4)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from datasets) (1.25.2)\n",
            "Requirement already satisfied: pyarrow>=12.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (14.0.2)\n",
            "Requirement already satisfied: pyarrow-hotfix in /usr/local/lib/python3.10/dist-packages (from datasets) (0.6)\n",
            "Collecting dill<0.3.9,>=0.3.0 (from datasets)\n",
            "  Downloading dill-0.3.8-py3-none-any.whl (116 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m15.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets) (2.0.3)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (2.31.0)\n",
            "Requirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (4.66.2)\n",
            "Collecting xxhash (from datasets)\n",
            "  Downloading xxhash-3.4.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (194 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.1/194.1 kB\u001b[0m \u001b[31m26.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting multiprocess (from datasets)\n",
            "  Downloading multiprocess-0.70.16-py310-none-any.whl (134 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.8/134.8 kB\u001b[0m \u001b[31m18.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: fsspec[http]<=2024.3.1,>=2023.1.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (2023.6.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets) (3.9.5)\n",
            "Collecting huggingface-hub>=0.21.2 (from datasets)\n",
            "  Downloading huggingface_hub-0.23.0-py3-none-any.whl (401 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m401.2/401.2 kB\u001b[0m \u001b[31m41.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from datasets) (24.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (6.0.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (23.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (6.0.5)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.9.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (4.0.3)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.21.2->datasets) (4.11.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (2024.2.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2023.4)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2024.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\n",
            "Installing collected packages: xxhash, dill, multiprocess, huggingface-hub, datasets\n",
            "  Attempting uninstall: huggingface-hub\n",
            "    Found existing installation: huggingface-hub 0.20.3\n",
            "    Uninstalling huggingface-hub-0.20.3:\n",
            "      Successfully uninstalled huggingface-hub-0.20.3\n",
            "Successfully installed datasets-2.19.0 dill-0.3.8 huggingface-hub-0.23.0 multiprocess-0.70.16 xxhash-3.4.1\n",
            "Collecting trl\n",
            "  Downloading trl-0.8.6-py3-none-any.whl (245 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m245.2/245.2 kB\u001b[0m \u001b[31m6.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: torch>=1.4.0 in /usr/local/lib/python3.10/dist-packages (from trl) (2.2.1+cu121)\n",
            "Requirement already satisfied: transformers>=4.31.0 in /usr/local/lib/python3.10/dist-packages (from trl) (4.40.1)\n",
            "Requirement already satisfied: numpy>=1.18.2 in /usr/local/lib/python3.10/dist-packages (from trl) (1.25.2)\n",
            "Collecting accelerate (from trl)\n",
            "  Downloading accelerate-0.29.3-py3-none-any.whl (297 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m297.6/297.6 kB\u001b[0m \u001b[31m37.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: datasets in /usr/local/lib/python3.10/dist-packages (from trl) (2.19.0)\n",
            "Collecting tyro>=0.5.11 (from trl)\n",
            "  Downloading tyro-0.8.3-py3-none-any.whl (102 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m102.0/102.0 kB\u001b[0m \u001b[31m17.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.4.0->trl) (3.13.4)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.4.0->trl) (4.11.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.4.0->trl) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.4.0->trl) (3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.4.0->trl) (3.1.3)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch>=1.4.0->trl) (2023.6.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.4.0->trl) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.4.0->trl) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.4.0->trl) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /usr/local/lib/python3.10/dist-packages (from torch>=1.4.0->trl) (8.9.2.26)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /usr/local/lib/python3.10/dist-packages (from torch>=1.4.0->trl) (12.1.3.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /usr/local/lib/python3.10/dist-packages (from torch>=1.4.0->trl) (11.0.2.54)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /usr/local/lib/python3.10/dist-packages (from torch>=1.4.0->trl) (10.3.2.106)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /usr/local/lib/python3.10/dist-packages (from torch>=1.4.0->trl) (11.4.5.107)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /usr/local/lib/python3.10/dist-packages (from torch>=1.4.0->trl) (12.1.0.106)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.19.3 in /usr/local/lib/python3.10/dist-packages (from torch>=1.4.0->trl) (2.19.3)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.4.0->trl) (12.1.105)\n",
            "Requirement already satisfied: triton==2.2.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.4.0->trl) (2.2.0)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.10/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch>=1.4.0->trl) (12.4.127)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.19.3 in /usr/local/lib/python3.10/dist-packages (from transformers>=4.31.0->trl) (0.23.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers>=4.31.0->trl) (24.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers>=4.31.0->trl) (6.0.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers>=4.31.0->trl) (2023.12.25)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers>=4.31.0->trl) (2.31.0)\n",
            "Requirement already satisfied: tokenizers<0.20,>=0.19 in /usr/local/lib/python3.10/dist-packages (from transformers>=4.31.0->trl) (0.19.1)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers>=4.31.0->trl) (0.4.3)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers>=4.31.0->trl) (4.66.2)\n",
            "Requirement already satisfied: docstring-parser>=0.14.1 in /usr/local/lib/python3.10/dist-packages (from tyro>=0.5.11->trl) (0.16)\n",
            "Requirement already satisfied: rich>=11.1.0 in /usr/local/lib/python3.10/dist-packages (from tyro>=0.5.11->trl) (13.7.1)\n",
            "Collecting shtab>=1.5.6 (from tyro>=0.5.11->trl)\n",
            "  Downloading shtab-1.7.1-py3-none-any.whl (14 kB)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from accelerate->trl) (5.9.5)\n",
            "Requirement already satisfied: pyarrow>=12.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets->trl) (14.0.2)\n",
            "Requirement already satisfied: pyarrow-hotfix in /usr/local/lib/python3.10/dist-packages (from datasets->trl) (0.6)\n",
            "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from datasets->trl) (0.3.8)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets->trl) (2.0.3)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from datasets->trl) (3.4.1)\n",
            "Requirement already satisfied: multiprocess in /usr/local/lib/python3.10/dist-packages (from datasets->trl) (0.70.16)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets->trl) (3.9.5)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets->trl) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets->trl) (23.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets->trl) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets->trl) (6.0.5)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets->trl) (1.9.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets->trl) (4.0.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers>=4.31.0->trl) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers>=4.31.0->trl) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers>=4.31.0->trl) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers>=4.31.0->trl) (2024.2.2)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich>=11.1.0->tyro>=0.5.11->trl) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich>=11.1.0->tyro>=0.5.11->trl) (2.16.1)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.4.0->trl) (2.1.5)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets->trl) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets->trl) (2023.4)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets->trl) (2024.1)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.4.0->trl) (1.3.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich>=11.1.0->tyro>=0.5.11->trl) (0.1.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas->datasets->trl) (1.16.0)\n",
            "Installing collected packages: shtab, tyro, accelerate, trl\n",
            "Successfully installed accelerate-0.29.3 shtab-1.7.1 trl-0.8.6 tyro-0.8.3\n",
            "Collecting peft\n",
            "  Downloading peft-0.10.0-py3-none-any.whl (199 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m199.1/199.1 kB\u001b[0m \u001b[31m4.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from peft) (1.25.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from peft) (24.0)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from peft) (5.9.5)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from peft) (6.0.1)\n",
            "Requirement already satisfied: torch>=1.13.0 in /usr/local/lib/python3.10/dist-packages (from peft) (2.2.1+cu121)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (from peft) (4.40.1)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from peft) (4.66.2)\n",
            "Requirement already satisfied: accelerate>=0.21.0 in /usr/local/lib/python3.10/dist-packages (from peft) (0.29.3)\n",
            "Requirement already satisfied: safetensors in /usr/local/lib/python3.10/dist-packages (from peft) (0.4.3)\n",
            "Requirement already satisfied: huggingface-hub>=0.17.0 in /usr/local/lib/python3.10/dist-packages (from peft) (0.23.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.17.0->peft) (3.13.4)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.17.0->peft) (2023.6.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.17.0->peft) (2.31.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.17.0->peft) (4.11.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.0->peft) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.0->peft) (3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.0->peft) (3.1.3)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.0->peft) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.0->peft) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.0->peft) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.0->peft) (8.9.2.26)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.0->peft) (12.1.3.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.0->peft) (11.0.2.54)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.0->peft) (10.3.2.106)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.0->peft) (11.4.5.107)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.0->peft) (12.1.0.106)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.19.3 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.0->peft) (2.19.3)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.0->peft) (12.1.105)\n",
            "Requirement already satisfied: triton==2.2.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.0->peft) (2.2.0)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.10/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch>=1.13.0->peft) (12.4.127)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers->peft) (2023.12.25)\n",
            "Requirement already satisfied: tokenizers<0.20,>=0.19 in /usr/local/lib/python3.10/dist-packages (from transformers->peft) (0.19.1)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.13.0->peft) (2.1.5)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.17.0->peft) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.17.0->peft) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.17.0->peft) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.17.0->peft) (2024.2.2)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.13.0->peft) (1.3.0)\n",
            "Installing collected packages: peft\n",
            "Successfully installed peft-0.10.0\n",
            "Requirement already satisfied: numba in /usr/local/lib/python3.10/dist-packages (0.58.1)\n",
            "Requirement already satisfied: llvmlite<0.42,>=0.41.0dev0 in /usr/local/lib/python3.10/dist-packages (from numba) (0.41.1)\n",
            "Requirement already satisfied: numpy<1.27,>=1.22 in /usr/local/lib/python3.10/dist-packages (from numba) (1.25.2)\n"
          ]
        }
      ],
      "source": [
        "!pip install causal-conv1d\n",
        "!pip install mamba-ssm\n",
        "!pip install datasets\n",
        "!pip install trl\n",
        "!pip install peft\n",
        "!pip install numba\n",
        "!mkdir /content/data\n",
        "!mkdir /content/mamba-chat\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EawJTTkXoOwt",
        "outputId": "d0e28843-3cef-422c-ce69-132c989b4cd7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "    drive.flush_and_unmount()\n",
        "    #drive.mount('/content/drive', force_remount=True)"
      ],
      "metadata": {
        "id": "IiaLre6D-pSa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WJYv3jUj8wGY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a723c533-fa9e-4ee4-d305-ea42a180fa0b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_token.py:89: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM, TrainingArguments, Trainer, MambaForCausalLM, MambaConfig\n",
        "from mamba_ssm.models.mixer_seq_simple import MambaLMHeadModel\n",
        "\n",
        "from datasets import load_dataset\n",
        "\n",
        "from trl import SFTTrainer, DataCollatorForCompletionOnlyLM\n",
        "from peft import LoraConfig\n",
        "import pandas as pd\n",
        "\n",
        "# from trainer.data import ChatDataModule\n",
        "# from trainer.mamba_trainer import MambaTrainer\n",
        "\n",
        "dataset = load_dataset(\"Kyudan/GTNT_PNT_WER\")\n",
        "dataset_chat = load_dataset(\"HuggingFaceH4/ultrachat_200k\")\n",
        "\n",
        "chatdataset = load_dataset(\"HuggingFaceH4/ultrachat_200k\", split=\"train_sft\")\n",
        "chatdataset_val = load_dataset(\"HuggingFaceH4/ultrachat_200k\", split=\"test_sft\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CWLK-a_CnCz8"
      },
      "outputs": [],
      "source": [
        "roleDict_base = {\n",
        "    \"content\":\"\", # Content here\n",
        "    \"role\":\"\", # Role here (assistant or user)\n",
        "}\n",
        "\n",
        "textlist = []\n",
        "data = dataset['train']\n",
        "\n",
        "data = data.remove_columns(\"WER of NT PNT\")\n",
        "\n",
        "train_1 = data['NT']\n",
        "train_2 = data['PNT']\n",
        "key = data[\"GT\"]\n",
        "for x in range(0, len(train_1)):\n",
        "  eq_list = []\n",
        "  roleDict = roleDict_base.copy()\n",
        "  roleDict[\"content\"] = train_1[x]\n",
        "  roleDict[\"role\"] = \"user\"\n",
        "  eq_list.append(roleDict)\n",
        "  roleDict = roleDict_base.copy()\n",
        "  roleDict[\"content\"] = key[x]\n",
        "  roleDict[\"role\"] = \"assistant\"\n",
        "  eq_list.append(roleDict)\n",
        "  textlist.append(eq_list)\n",
        "  eq_list = []\n",
        "  roleDict = roleDict_base.copy()\n",
        "  roleDict[\"content\"] = train_2[x]\n",
        "  roleDict[\"role\"] = \"user\"\n",
        "  eq_list.append(roleDict)\n",
        "  roleDict = roleDict_base.copy()\n",
        "  roleDict[\"content\"] = key[x]\n",
        "  roleDict[\"role\"] = \"assistant\"\n",
        "  eq_list.append(roleDict)\n",
        "  textlist.append(eq_list)\n",
        "\n",
        "\n",
        "messages_dataset = {\"messages\": textlist}\n",
        "dataset_test = Dataset.from_dict(messages_dataset)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lstULRyA7YbH"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import transformers\n",
        "import json\n",
        "\n",
        "from dataclasses import dataclass\n",
        "from typing import Dict, Sequence\n",
        "from tqdm import tqdm\n",
        "from torch.utils.data import Dataset\n",
        "\n",
        "\n",
        "class ChatDataset(Dataset):\n",
        "    def __init__(self, data_path: str, tokenizer: transformers.AutoTokenizer, conversation_template: str, max_tokens: int):\n",
        "        super(ChatDataset, self).__init__()\n",
        "        data = []\n",
        "        with open(data_path, \"r\") as file:\n",
        "            for line in file:\n",
        "                try:\n",
        "                    data.append(json.loads(line))\n",
        "                except Exception as e:\n",
        "                    print(\"json processing exception\", e)\n",
        "                    continue\n",
        "\n",
        "\n",
        "        data_dict = preprocess(data, tokenizer, conversation_template, max_tokens)\n",
        "\n",
        "        self.input_ids = data_dict[\"input_ids\"]\n",
        "        self.labels = data_dict[\"labels\"]\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.input_ids)\n",
        "\n",
        "    def __getitem__(self, i) -> Dict[str, torch.Tensor]:\n",
        "        return dict(input_ids=self.input_ids[i], labels=self.labels[i])\n",
        "\n",
        "\n",
        "@dataclass\n",
        "class DataCollatorForChatDataset(object):\n",
        "    \"\"\"\n",
        "    Collate examples for supervised fine-tuning.\n",
        "    \"\"\"\n",
        "\n",
        "    tokenizer: transformers.PreTrainedTokenizer\n",
        "\n",
        "    def __call__(self, instances: Sequence[Dict]) -> Dict[str, torch.Tensor]:\n",
        "        input_ids, labels = tuple([instance[key] for instance in instances] for key in (\"input_ids\", \"input_ids\"))\n",
        "        input_ids = torch.nn.utils.rnn.pad_sequence(input_ids, batch_first=True, padding_value=self.tokenizer.pad_token_id)\n",
        "        labels = torch.nn.utils.rnn.pad_sequence(labels, batch_first=True, padding_value=-100)\n",
        "\n",
        "        return dict(\n",
        "            input_ids=input_ids,\n",
        "            labels=labels,\n",
        "            attention_mask=input_ids.ne(self.tokenizer.pad_token_id),\n",
        "        )\n",
        "\n",
        "\n",
        "class ChatDataModule():\n",
        "    def __init__(self, tokenizer: transformers.PreTrainedTokenizer, data_path: str, conversation_template, max_tokens: int):\n",
        "\n",
        "        self.dataset = ChatDataset(tokenizer=tokenizer, data_path=data_path, conversation_template=conversation_template, max_tokens=max_tokens)\n",
        "        self.data_collator = DataCollatorForChatDataset(tokenizer=tokenizer)\n",
        "\n",
        "\n",
        "def preprocess(conversations: Sequence[Sequence[dict]], tokenizer: transformers.PreTrainedTokenizer, conversation_template: str, max_tokens: int) -> Dict:\n",
        "    \"\"\"\n",
        "    Preprocess the data by tokenizing.\n",
        "    \"\"\"\n",
        "    all_input_ids = []\n",
        "    all_label_ids = []\n",
        "    tokenizer.use_default_system_prompt = False\n",
        "\n",
        "    print(\"Tokenizing dataset...\")\n",
        "    for conv in tqdm(conversations):\n",
        "        current_conv = conv[\"messages\"]\n",
        "        tokenized_responses = []\n",
        "        for msg in current_conv:\n",
        "            if msg[\"role\"] == \"assistant\":\n",
        "                tokenized_responses.append(tokenizer.encode(msg[\"content\"], add_special_tokens=False))\n",
        "\n",
        "        tokenized_conv = tokenizer.apply_chat_template(current_conv, chat_template=conversation_template, max_length=max_tokens, truncation=True)\n",
        "        all_input_ids.append(torch.LongTensor(tokenized_conv))\n",
        "\n",
        "\n",
        "    return dict(input_ids=all_input_ids, labels=all_input_ids)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QYrbMyy4qYwK"
      },
      "outputs": [],
      "source": [
        "with open(\"/content/data/math.jsonl\", \"w\") as f:\n",
        "    for d in dataset_test:\n",
        "        f.write(json.dumps(dict(messages=d[\"messages\"]))+\"\\n\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 424
        },
        "id": "2Jyc6vS5qFiC",
        "outputId": "0b6248a6-a671-4e06-c99d-30f0d105a262"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "dataf"
            },
            "text/html": [
              "\n",
              "  <div id=\"df-dc76a73e-fff0-417c-b8dd-9bf0b40808a1\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Input</th>\n",
              "      <th>Output</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>given the formula open curly bracket a to the ...</td>\n",
              "      <td>Given the formula $[ \\{ a^{ - } , a^{ + } \\} ,...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>given the formula open curly bracket a to the...</td>\n",
              "      <td>Given the formula $[ \\{ a^{ - } , a^{ + } \\} ,...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>when it comes to l c m of a g c d of b c equal...</td>\n",
              "      <td>When it comes to $\\operatorname { l c m } ( a ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>when it comes to lcm of a gc div b c equals g...</td>\n",
              "      <td>When it comes to $\\operatorname { l c m } ( a ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>when it comes to f sub u t equals f sub u minu...</td>\n",
              "      <td>When it comes to $\\operatorname { F }_{ u , t ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>186843</th>\n",
              "      <td>in the context of the law the equation open p...</td>\n",
              "      <td>In the context of the law, the equation $( { \\...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>186844</th>\n",
              "      <td>when it comes to math bold i squared equals ma...</td>\n",
              "      <td>When it comes to $\\mathbf { i }^{ 2 } = \\mathb...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>186845</th>\n",
              "      <td>when it comes to math bold i squared equals m...</td>\n",
              "      <td>When it comes to $\\mathbf { i }^{ 2 } = \\mathb...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>186846</th>\n",
              "      <td>consider the equation delta g equals delta g n...</td>\n",
              "      <td>Consider the equation $\\DeltaG = \\DeltaG?? + R...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>186847</th>\n",
              "      <td>consider the equation delta g equals delta g ...</td>\n",
              "      <td>Consider the equation $\\DeltaG = \\DeltaG?? + R...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>186848 rows × 2 columns</p>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-dc76a73e-fff0-417c-b8dd-9bf0b40808a1')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-dc76a73e-fff0-417c-b8dd-9bf0b40808a1 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-dc76a73e-fff0-417c-b8dd-9bf0b40808a1');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-f709bec6-e4a5-46a6-ba26-85bf42711ed1\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-f709bec6-e4a5-46a6-ba26-85bf42711ed1')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-f709bec6-e4a5-46a6-ba26-85bf42711ed1 button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "text/plain": [
              "                                                    Input  \\\n",
              "0       given the formula open curly bracket a to the ...   \n",
              "1        given the formula open curly bracket a to the...   \n",
              "2       when it comes to l c m of a g c d of b c equal...   \n",
              "3        when it comes to lcm of a gc div b c equals g...   \n",
              "4       when it comes to f sub u t equals f sub u minu...   \n",
              "...                                                   ...   \n",
              "186843   in the context of the law the equation open p...   \n",
              "186844  when it comes to math bold i squared equals ma...   \n",
              "186845   when it comes to math bold i squared equals m...   \n",
              "186846  consider the equation delta g equals delta g n...   \n",
              "186847   consider the equation delta g equals delta g ...   \n",
              "\n",
              "                                                   Output  \n",
              "0       Given the formula $[ \\{ a^{ - } , a^{ + } \\} ,...  \n",
              "1       Given the formula $[ \\{ a^{ - } , a^{ + } \\} ,...  \n",
              "2       When it comes to $\\operatorname { l c m } ( a ...  \n",
              "3       When it comes to $\\operatorname { l c m } ( a ...  \n",
              "4       When it comes to $\\operatorname { F }_{ u , t ...  \n",
              "...                                                   ...  \n",
              "186843  In the context of the law, the equation $( { \\...  \n",
              "186844  When it comes to $\\mathbf { i }^{ 2 } = \\mathb...  \n",
              "186845  When it comes to $\\mathbf { i }^{ 2 } = \\mathb...  \n",
              "186846  Consider the equation $\\DeltaG = \\DeltaG?? + R...  \n",
              "186847  Consider the equation $\\DeltaG = \\DeltaG?? + R...  \n",
              "\n",
              "[186848 rows x 2 columns]"
            ]
          },
          "execution_count": 3,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "data = dataset['train']\n",
        "\n",
        "data = data.remove_columns(\"WER of NT PNT\")\n",
        "\n",
        "# GT key\n",
        "# NT and PNT are 2 different ways to explain the equation\n",
        "\n",
        "train_1 = data['NT']\n",
        "train_2 = data['PNT']\n",
        "key = data[\"GT\"]\n",
        "\n",
        "datad = {}\n",
        "\n",
        "for x in range(0, len(train_1)):\n",
        "  datad[train_1[x]] = key[x]\n",
        "  datad[train_2[x]] = key[x]\n",
        "\n",
        "\n",
        "input_list = list(datad.keys())\n",
        "\n",
        "output_list = []\n",
        "for x in input_list:\n",
        "  output_list.append(datad[x])\n",
        "\n",
        "\n",
        "\n",
        "dataf = pd.DataFrame({\"Input\":input_list, \"Output\":output_list})\n",
        "\n",
        "dataf\n",
        "\n",
        "#datasetm = Dataset.from_pandas(dataf)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fUq8AfsEYn18"
      },
      "outputs": [],
      "source": [
        "from transformers import Trainer\n",
        "import torch\n",
        "import os\n",
        "\n",
        "\n",
        "class MambaTrainer(Trainer):\n",
        "    def compute_loss(self, model, inputs, return_outputs=False):\n",
        "        input_ids = inputs.pop(\"input_ids\")\n",
        "        lm_logits = model(input_ids).logits\n",
        "\n",
        "        labels = input_ids.to(lm_logits.device)\n",
        "        shift_logits = lm_logits[:, :-1, :].contiguous()\n",
        "        labels = labels[:, 1:].contiguous()\n",
        "\n",
        "        loss_fct = torch.nn.CrossEntropyLoss()\n",
        "        lm_loss = loss_fct(shift_logits.view(-1, shift_logits.size(-1)), labels.view(-1))\n",
        "\n",
        "        return lm_loss\n",
        "\n",
        "    def save_model(self, output_dir, _internal_call):\n",
        "        if not os.path.exists(output_dir):\n",
        "            os.makedirs(output_dir)\n",
        "        #second_output_dir=\"/root/nextcloud/Documents/NLP_Project\"\n",
        "        torch.save(self.model.state_dict(), f\"{output_dir}/pytorch_model.bin\")\n",
        "        #torch.save(self.model.state_dict(), f\"{second_output_dir}/pytorch_model.bin\")\n",
        "        model.save_pretrained(output_dir)\n",
        "        self.tokenizer.save_pretrained(output_dir)\n",
        "        #self.tokenizer.save_pretrained(second_output_dir)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d2O7YxvknPQ2"
      },
      "outputs": [],
      "source": [
        "#model_id = \"havenhq/mamba-chat\"\n",
        "#model_id = \"state-spaces/mamba-130m-hf\"\n",
        "#state-spaces/mamba-790m-hf\n",
        "#model_id = \"state-spaces/mamba-1.4b-hf\"\n",
        "model_id = \"state-spaces/mamba-790m-hf\"\n",
        "#model_id2 = \"state-spaces/mamba-2.8b-hf\"\n",
        "#tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
        "#model = MambaLMHeadModel.from_pretrained(\"state-spaces/mamba-1.4b-hf\", device=\"cuda\", dtype=torch.float16)\n",
        "\n",
        "model = AutoModelForCausalLM.from_pretrained(model_id, residual_in_fp32 = False)\n",
        "#model.config.to_json_file(\"/content/drive/MyDrive/NLP_Project/mamba-chat-4/checkpoint-2672/config.json\")\n",
        "#model2 = AutoModelForCausalLM.from_pretrained(model_id2)\n",
        "#model = MambaLMHeadModel.from_pretrained(\"havenhq/mamba-chat\", device=\"cuda\", dtype=torch.float16)\n",
        "\n",
        "#Train = MambaTrainer(model_id, datasetm)\n",
        "#model = MambaLMHeadModel.from_pretrained(\"havenhq/mamba-chat\", device=\"cuda\", dtype=torch.float16)\n",
        "#python train_mamba.py --model state-spaces/mamba-2.8b --tokenizer EleutherAI/gpt-neox-20b --learning_rate 5e-5 --batch_size 4 --data_path ./data/ultrachat_small.jsonl --num_epochs 3\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-uU7EGHstcWv"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "#model.load_state_dict(torch.load(\"pytorch_model.bin\"))\n",
        "#rng_state = torch.load(\"rng_state.pth\")\n",
        "#torch.set_rng_state(rng_state)\n",
        "#model = AutoModelForCausalLM.from_pretrained(\"/content/drive/MyDrive/NLP_Project/mamba-chat-4/checkpoint-2672/\")\n",
        "#model = MambaLMHeadModel.from_pretrained(\"/content/drive/MyDrive/NLP_Project/mamba-chat-4/checkpoint-2672/py\", dtype=torch.bfloat16, device=\"cuda\")\n",
        "\n",
        "#model = AutoModelForCausalLM.from_pretrained(\"/content/drive/MyDrive/NLP_Project/mamba-chat-4/checkpoint-2672/\", residual_in_fp32 = False)\n",
        "\n",
        "model = AutoModelForCausalLM.from_pretrained(\"/content/drive/MyDrive/NLP_Project/mamba-chat-math2/checkpoint-7800\", residual_in_fp32 = False)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aTEwYqvc-AKJ"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "from datasets import load_dataset\n",
        "\n",
        "data = load_dataset(\"HuggingFaceH4/ultrachat_200k\", split=\"train_sft\")\n",
        "\n",
        "\n",
        "with open(\"/content/data/ultrachat.jsonl\", \"w\") as f:\n",
        "    for d in data:\n",
        "        f.write(json.dumps(dict(messages=d[\"messages\"]))+\"\\n\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ThPtWjTa9eGJ"
      },
      "outputs": [],
      "source": [
        "\n",
        "torch.cuda.empty_cache()\n",
        "\n",
        "# from numba import cuda\n",
        "# device = cuda.get_current_device()\n",
        "# device.reset()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ACEzB2-k7r1r",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "23c0b4a3-0fed-4429-a0ed-9fa2e0929b73"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
            "  warnings.warn(\n",
            "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
          ]
        }
      ],
      "source": [
        "#model = MambaLMHeadModel.from_pretrained(model_id, dtype=torch.bfloat16, device=\"CPU\")\n",
        "#model_id = \"state-spaces/mamba-790m-hf\"\n",
        "#model_id = \"state-spaces/mamba-2.8b\"\n",
        "#model = AutoModelForCausalLM.from_pretrained(model_id)\n",
        "#model = MambaLMHeadModel.from_pretrained(model_id, device=\"cuda\", dtype=torch.float16)\n",
        "#model = MambaLMHeadModel.from_pretrained(model_id, dtype=torch.bfloat16, device=\"cuda\")\n",
        "\n",
        "\n",
        "#model = MambaLMHeadModel.from_pretrained(\"havenhq/mamba-chat\", device=\"cuda\", dtype=torch.float16)\n",
        "#\"EleutherAI/gpt-neo-1.3B\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"EleutherAI/gpt-neox-20b\")\n",
        "#tokenizer = AutoTokenizer.from_pretrained(\"EleutherAI/gpt-neo-1.3B\")\n",
        "\n",
        "tokenizer.eos_token = \"<|endoftext|>\"\n",
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "tokenizer.chat_template = AutoTokenizer.from_pretrained(\"HuggingFaceH4/zephyr-7b-beta\").chat_template\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gPsoeeDbqe7H",
        "outputId": "23d929b4-6703-4082-c7b9-b48196cb84fc"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Tokenizing dataset...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 187236/187236 [01:50<00:00, 1697.17it/s]\n"
          ]
        }
      ],
      "source": [
        "data_module_math = ChatDataModule(\n",
        "        tokenizer=tokenizer,\n",
        "        data_path=\"/content/data/math.jsonl\",\n",
        "        conversation_template=tokenizer.chat_template,\n",
        "        max_tokens=2048\n",
        "        #max_tokens=1024\n",
        "    )\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vRUVHROkjttT"
      },
      "outputs": [],
      "source": [
        "#!rsync -r /root/nextcloud/Documents/NLP_Project/data_module.pkl /content/data_module_1.pkl\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "F5iKcT4EERL7"
      },
      "outputs": [],
      "source": [
        "import pickle\n",
        "# dir(data_module)\n",
        "# data_module.dataset\n",
        "def save_data_module(data_module, filepath):\n",
        "    with open(filepath, 'wb') as f:\n",
        "        pickle.dump(data_module, f)\n",
        "\n",
        "# Load ChatDataModule instance\n",
        "def load_data_module(filepath):\n",
        "    with open(filepath, 'rb') as f:\n",
        "        data_module = pickle.load(f)\n",
        "    return data_module\n",
        "\n",
        "#save_data_module(data_module_math, '/content/drive/MyDrive/NLP_Project/data_module_math.pkl')\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "03hxOlQgzgqf"
      },
      "outputs": [],
      "source": [
        "data_module = load_data_module(\"/content/data_module_small.pkl\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "C4oqGDhwLPPj"
      },
      "outputs": [],
      "source": [
        "#data_module = load_data_module(\"/content/data_module.pkl\")\n",
        "#data_module = load_data_module(\"/content/drive/MyDrive/NLP_Project/data_module.pkl\")\n",
        "data_module_math = load_data_module(\"/content/drive/MyDrive/NLP_Project/data_module_math.pkl\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HshME662-0lY"
      },
      "outputs": [],
      "source": [
        "data_module = ChatDataModule(\n",
        "        tokenizer=tokenizer,\n",
        "        data_path=\"/content/data/ultrachat.jsonl\",\n",
        "        conversation_template=tokenizer.chat_template,\n",
        "        #max_tokens=2048\n",
        "        max_tokens=1024\n",
        "    )\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-r-fWx7470Wi",
        "outputId": "e5570b83-01c6-48b7-8b46-6ed7218ee6ae"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "('/content/data/tokenizer_config.json',\n",
              " '/content/data/special_tokens_map.json',\n",
              " '/content/data/tokenizer.json')"
            ]
          },
          "execution_count": 20,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "#tokenizer.save_pretrained(\"/content/data\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hfe03A11_kP1"
      },
      "outputs": [],
      "source": [
        "trainer = MambaTrainer(\n",
        "    model=model,\n",
        "    train_dataset=data_module_math.dataset,\n",
        "    tokenizer=tokenizer,\n",
        "    args=TrainingArguments(\n",
        "        learning_rate= 5e-5,\n",
        "        num_train_epochs= 1,\n",
        "        per_device_train_batch_size = 8,\n",
        "        gradient_accumulation_steps = 1,\n",
        "        optim = \"adamw_torch\",\n",
        "        logging_dir=\"/content/drive/MyDrive/NLP_Project/mamba-chat-math2\",\n",
        "        output_dir=\"/content/drive/MyDrive/NLP_Project/mamba-chat-math2\",\n",
        "        #logging_dir=\"/content/mamba-chat\",\n",
        "        #output_dir=\"/content/mamba-chat\",\n",
        "        logging_steps=20,\n",
        "        save_steps=200,\n",
        "        save_total_limit=5,\n",
        "    ),\n",
        "    data_collator=data_module_math.data_collator,\n",
        ")\n",
        "with open('/content/drive/MyDrive/NLP_Project/mamba-chat-math2/checkpoint-7800/trainer_state.json', 'r') as f:\n",
        "    trainer_state = json.load(f)\n",
        "# Set the trainer state\n",
        "trainer.current_epoch = trainer_state['epoch']\n",
        "trainer.global_step = trainer_state['global_step']\n",
        "trainer.state.log_history = trainer_state['log_history']\n",
        "trainer.state.best_metric = trainer_state['best_metric']\n",
        "trainer.state.best_model_checkpoint = trainer_state['best_model_checkpoint']\n",
        "\n",
        "# Load the model and optimizer state\n",
        "#trainer.optimizer.load_state_dict(torch.load('/content/drive/MyDrive/NLP_Project/mamba-chat-math/checkpoint-600/optimizer.pt'))\n",
        "#model.load_state_dict(trainer_state['model_state_dict'])\n",
        "#trainer.optimizer.load_state_dict(trainer_state['optimizer_state_dict'])\n",
        "\n",
        "\n",
        "\n",
        "# trainer.train()\n",
        "\n",
        "# dir(model)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#trainer._load_from_checkpoint(\"/content/drive/MyDrive/NLP_Project/mamba-chat-math2/checkpoint-1200\")\n",
        "trainer.train(resume_from_checkpoint=\"/content/drive/MyDrive/NLP_Project/mamba-chat-math2/checkpoint-7800\")"
      ],
      "metadata": {
        "id": "LhkmWn-f6nNz",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "de91a2cd-f0a3-438c-e1cd-37183c4df919"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='10024' max='23405' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [10024/23405 47:19 < 4:44:59, 0.78 it/s, Epoch 0.43/1]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>7820</td>\n",
              "      <td>0.462100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>7840</td>\n",
              "      <td>0.452500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>7860</td>\n",
              "      <td>0.463600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>7880</td>\n",
              "      <td>0.465400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>7900</td>\n",
              "      <td>0.447300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>7920</td>\n",
              "      <td>0.416600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>7940</td>\n",
              "      <td>0.483100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>7960</td>\n",
              "      <td>0.455700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>7980</td>\n",
              "      <td>0.440500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>8000</td>\n",
              "      <td>0.464900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>8020</td>\n",
              "      <td>0.472600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>8040</td>\n",
              "      <td>0.432400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>8060</td>\n",
              "      <td>0.430300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>8080</td>\n",
              "      <td>0.456000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>8100</td>\n",
              "      <td>0.458800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>8120</td>\n",
              "      <td>0.454900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>8140</td>\n",
              "      <td>0.471500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>8160</td>\n",
              "      <td>0.436700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>8180</td>\n",
              "      <td>0.437800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>8200</td>\n",
              "      <td>0.473900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>8220</td>\n",
              "      <td>0.447400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>8240</td>\n",
              "      <td>0.471600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>8260</td>\n",
              "      <td>0.441400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>8280</td>\n",
              "      <td>0.471700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>8300</td>\n",
              "      <td>0.456900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>8320</td>\n",
              "      <td>0.429700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>8340</td>\n",
              "      <td>0.425500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>8360</td>\n",
              "      <td>0.453400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>8380</td>\n",
              "      <td>0.411000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>8400</td>\n",
              "      <td>0.505000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>8420</td>\n",
              "      <td>0.444800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>8440</td>\n",
              "      <td>0.477100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>8460</td>\n",
              "      <td>0.490000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>8480</td>\n",
              "      <td>0.447400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>8500</td>\n",
              "      <td>0.440400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>8520</td>\n",
              "      <td>0.455400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>8540</td>\n",
              "      <td>0.436900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>8560</td>\n",
              "      <td>0.413600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>8580</td>\n",
              "      <td>0.450100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>8600</td>\n",
              "      <td>0.470400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>8620</td>\n",
              "      <td>0.433100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>8640</td>\n",
              "      <td>0.472600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>8660</td>\n",
              "      <td>0.429400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>8680</td>\n",
              "      <td>0.454800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>8700</td>\n",
              "      <td>0.465500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>8720</td>\n",
              "      <td>0.429000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>8740</td>\n",
              "      <td>0.459900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>8760</td>\n",
              "      <td>0.453300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>8780</td>\n",
              "      <td>0.469300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>8800</td>\n",
              "      <td>0.475300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>8820</td>\n",
              "      <td>0.430500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>8840</td>\n",
              "      <td>0.425500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>8860</td>\n",
              "      <td>0.451300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>8880</td>\n",
              "      <td>0.452600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>8900</td>\n",
              "      <td>0.465600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>8920</td>\n",
              "      <td>0.458300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>8940</td>\n",
              "      <td>0.435000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>8960</td>\n",
              "      <td>0.442500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>8980</td>\n",
              "      <td>0.438300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>9000</td>\n",
              "      <td>0.436600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>9020</td>\n",
              "      <td>0.440300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>9040</td>\n",
              "      <td>0.464100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>9060</td>\n",
              "      <td>0.458100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>9080</td>\n",
              "      <td>0.469500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>9100</td>\n",
              "      <td>0.448900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>9120</td>\n",
              "      <td>0.430400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>9140</td>\n",
              "      <td>0.460500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>9160</td>\n",
              "      <td>0.454700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>9180</td>\n",
              "      <td>0.422700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>9200</td>\n",
              "      <td>0.397500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>9220</td>\n",
              "      <td>0.409200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>9240</td>\n",
              "      <td>0.461100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>9260</td>\n",
              "      <td>0.437500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>9280</td>\n",
              "      <td>0.475200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>9300</td>\n",
              "      <td>0.451700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>9320</td>\n",
              "      <td>0.459400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>9340</td>\n",
              "      <td>0.449700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>9360</td>\n",
              "      <td>0.414600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>9380</td>\n",
              "      <td>0.431300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>9400</td>\n",
              "      <td>0.439700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>9420</td>\n",
              "      <td>0.416200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>9440</td>\n",
              "      <td>0.426800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>9460</td>\n",
              "      <td>0.407400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>9480</td>\n",
              "      <td>0.435400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>9500</td>\n",
              "      <td>0.465600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>9520</td>\n",
              "      <td>0.436600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>9540</td>\n",
              "      <td>0.438300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>9560</td>\n",
              "      <td>0.448400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>9580</td>\n",
              "      <td>0.476000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>9600</td>\n",
              "      <td>0.417200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>9620</td>\n",
              "      <td>0.456400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>9640</td>\n",
              "      <td>0.451200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>9660</td>\n",
              "      <td>0.448200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>9680</td>\n",
              "      <td>0.445100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>9700</td>\n",
              "      <td>0.453300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>9720</td>\n",
              "      <td>0.398200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>9740</td>\n",
              "      <td>0.452600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>9760</td>\n",
              "      <td>0.440000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>9780</td>\n",
              "      <td>0.427000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>9800</td>\n",
              "      <td>0.465700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>9820</td>\n",
              "      <td>0.370100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>9840</td>\n",
              "      <td>0.457400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>9860</td>\n",
              "      <td>0.438900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>9880</td>\n",
              "      <td>0.417100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>9900</td>\n",
              "      <td>0.445700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>9920</td>\n",
              "      <td>0.405000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>9940</td>\n",
              "      <td>0.426600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>9960</td>\n",
              "      <td>0.436500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>9980</td>\n",
              "      <td>0.406900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>10000</td>\n",
              "      <td>0.406100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>10020</td>\n",
              "      <td>0.483100</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-9-f06d1c4a57ed>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m#trainer._load_from_checkpoint(\"/content/drive/MyDrive/NLP_Project/mamba-chat-math2/checkpoint-1200\")\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresume_from_checkpoint\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"/content/drive/MyDrive/NLP_Project/mamba-chat-math2/checkpoint-7800\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/trainer.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   1857\u001b[0m                 \u001b[0mhf_hub_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menable_progress_bars\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1858\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1859\u001b[0;31m             return inner_training_loop(\n\u001b[0m\u001b[1;32m   1860\u001b[0m                 \u001b[0margs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1861\u001b[0m                 \u001b[0mresume_from_checkpoint\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mresume_from_checkpoint\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/trainer.py\u001b[0m in \u001b[0;36m_inner_training_loop\u001b[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   2201\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2202\u001b[0m                 \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maccelerator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maccumulate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2203\u001b[0;31m                     \u001b[0mtr_loss_step\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtraining_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2204\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2205\u001b[0m                 if (\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/trainer.py\u001b[0m in \u001b[0;36mtraining_step\u001b[0;34m(self, model, inputs)\u001b[0m\n\u001b[1;32m   3145\u001b[0m                 \u001b[0mscaled_loss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3146\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3147\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maccelerator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3148\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3149\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgradient_accumulation_steps\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/accelerate/accelerator.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, loss, **kwargs)\u001b[0m\n\u001b[1;32m   2011\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscaler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscale\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2012\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2013\u001b[0;31m             \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2014\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2015\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mset_trigger\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    520\u001b[0m                 \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    521\u001b[0m             )\n\u001b[0;32m--> 522\u001b[0;31m         torch.autograd.backward(\n\u001b[0m\u001b[1;32m    523\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    524\u001b[0m         )\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    264\u001b[0m     \u001b[0;31m# some Python versions print out the first line of a multi-line function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    265\u001b[0m     \u001b[0;31m# calls in the traceback and some print out the last line\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 266\u001b[0;31m     Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n\u001b[0m\u001b[1;32m    267\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    268\u001b[0m         \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6tKE3s3lp_UJ"
      },
      "outputs": [],
      "source": [
        "device = \"cuda\"\n",
        "#tokenizer = AutoTokenizer.from_pretrained(\"havenhq/mamba-chat\")\n",
        "tokenizer.eos_token = \"<|endoftext|>\"\n",
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "tokenizer.chat_template = AutoTokenizer.from_pretrained(\"HuggingFaceH4/zephyr-7b-beta\").chat_template\n",
        "\n",
        "#model = MambaLMHeadModel.from_pretrained(\"havenhq/mamba-chat\", device=\"cuda\", dtype=torch.float16)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xYChz2k79OlG"
      },
      "outputs": [],
      "source": [
        "dir(model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LWzug5q-9XZ-"
      },
      "outputs": [],
      "source": [
        "messages = []\n",
        "while True:\n",
        "    user_message = input(\"\\nYour message: \")\n",
        "    messages.append(dict(\n",
        "        role=\"user\",\n",
        "        content=user_message\n",
        "    ))\n",
        "\n",
        "    input_ids = tokenizer.apply_chat_template(messages, return_tensors=\"pt\", add_generation_prompt=True).to(\"cuda\")\n",
        "\n",
        "    out = model.generate(input_ids=input_ids, max_length=2000, temperature=0.9, top_p=0.7, eos_token_id=tokenizer.eos_token_id)\n",
        "\n",
        "    decoded = tokenizer.batch_decode(out)\n",
        "    messages.append(dict(\n",
        "        role=\"assistant\",\n",
        "        content=decoded[0].split(\"<|assistant|>\\n\")[-1])\n",
        "    )\n",
        "\n",
        "    print(\"Model:\", decoded[0].split(\"<|assistant|>\\n\")[-1])"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "machine_shape": "hm",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}