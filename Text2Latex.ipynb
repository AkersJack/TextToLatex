{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AkersJack/TextToLatex/blob/main/Text2Latex.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YzzWrhZVVNgM"
      },
      "outputs": [],
      "source": [
        "!pip install mamba-ssm\n",
        "!pip install datasets\n",
        "!pip install trl\n",
        "!pip install peft\n",
        "!pip install numba"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7RmuDWW7ViKm",
        "outputId": "64585dab-4216-488c-fca6-65bab831fcba"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1ZWK8MNbUvup"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM, TrainingArguments, Trainer, MambaForCausalLM, MambaConfig\n",
        "\n",
        "from mamba_ssm.models.mixer_seq_simple import MambaLMHeadModel\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m3V4Y7FhVA09",
        "outputId": "383f33b9-8cf9-4370-cdbf-ff708267fabb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
            "The fast path is not available because on of `(selective_state_update, selective_scan_fn, causal_conv1d_fn, causal_conv1d_update, mamba_inner_fn)` is None. Falling back to the naive implementation. To install follow https://github.com/state-spaces/mamba/#installation and https://github.com/Dao-AILab/causal-conv1d\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "MambaForCausalLM(\n",
              "  (backbone): MambaModel(\n",
              "    (embeddings): Embedding(50280, 1536)\n",
              "    (layers): ModuleList(\n",
              "      (0-47): 48 x MambaBlock(\n",
              "        (norm): MambaRMSNorm()\n",
              "        (mixer): MambaMixer(\n",
              "          (conv1d): Conv1d(3072, 3072, kernel_size=(4,), stride=(1,), padding=(3,), groups=3072)\n",
              "          (act): SiLU()\n",
              "          (in_proj): Linear(in_features=1536, out_features=6144, bias=False)\n",
              "          (x_proj): Linear(in_features=3072, out_features=128, bias=False)\n",
              "          (dt_proj): Linear(in_features=96, out_features=3072, bias=True)\n",
              "          (out_proj): Linear(in_features=3072, out_features=1536, bias=False)\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (norm_f): MambaRMSNorm()\n",
              "  )\n",
              "  (lm_head): Linear(in_features=1536, out_features=50280, bias=False)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ],
      "source": [
        "device = \"cuda\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"/content/drive/MyDrive/NLP_Project/mamba-chat-math2/checkpoint-8000\")\n",
        "#tokenizer = AutoTokenizer.from_pretrained(\"havenhq/mamba-chat\")\n",
        "\n",
        "tokenizer.eos_token = \"<|endoftext|>\"\n",
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "tokenizer.chat_template = AutoTokenizer.from_pretrained(\"HuggingFaceH4/zephyr-7b-beta\").chat_template\n",
        "\n",
        "model = AutoModelForCausalLM.from_pretrained(\"/content/drive/MyDrive/NLP_Project/mamba-chat-math2/checkpoint-8000\", residual_in_fp32 = False)\n",
        "#model.to(\"cuda\")\n",
        "#model = MambaLMHeadModel.from_pretrained(\"havenhq/mamba-chat\", device=\"cuda\", dtype=torch.float16)\n",
        "model.to(\"cuda\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-s_Ti70jVXLm"
      },
      "source": [
        "Chat with Mamba"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3C3cItZCVWlx"
      },
      "outputs": [],
      "source": [
        "messages = []\n",
        "while True:\n",
        "    user_message = input(\"\\nYour message: \")\n",
        "    messages.append(dict(\n",
        "        role=\"user\",\n",
        "        content=user_message\n",
        "    ))\n",
        "\n",
        "    input_ids = tokenizer.apply_chat_template(messages, return_tensors=\"pt\", add_generation_prompt=True).to(\"cuda\")\n",
        "    #input_ids = tokenizer.apply_chat_template(messages, return_tensors=\"pt\", add_generation_prompt=True)\n",
        "\n",
        "    #out = model.generate(input_ids=input_ids, max_length=2000, temperature=0.9, top_p=0.7, eos_token_id=tokenizer.eos_token_id)\n",
        "    out = model.generate(input_ids=input_ids, max_length=8000, temperature=0.9, top_p=0.7, eos_token_id=tokenizer.eos_token_id)\n",
        "\n",
        "    decoded = tokenizer.batch_decode(out)\n",
        "    messages.append(dict(\n",
        "        role=\"assistant\",\n",
        "        content=decoded[0].split(\"<|assistant|>\\n\")[-1])\n",
        "    )\n",
        "    output = decoded[0].split(\"<|assistant|>\\n\")[-1]\n",
        "    print(\"Model:\", output.split(\"<|endoftext|>\")[0])"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyPz2n6s+1F8/NQFdIqee2oN",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}